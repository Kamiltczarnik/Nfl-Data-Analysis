## Project Architecture and Module Contracts

This document maps how modules interact, what each produces/consumes, and the key interfaces and file formats.

### High-level data flow

1) Data ingestion (nflreadpy, optional scrapes) → Parquet lake under `data/` (partitioned by `season`/`week`).
2) Feature engineering from Parquet inputs → modeling table (per team-game row) in Parquet and/or SQLite/Postgres feature store.
3) Modeling (baseline logistic, GBM, stack) → serialized model artifacts + calibration mappings.
4) Serving via CLI (`src/serve/predict.py`) → load features, apply overrides, run inference, output probabilities + explanations.
5) Evaluation harness (`src/eval/*`) → time-series CV, reliability curves, ablations.

6) API layer (`src/api/*`) → exposes read-only slate browsing and prediction endpoints that call serving/inference.

### Configs and overrides

- `configs/paths.yaml`: storage locations for raw snapshots, parquet lake, feature store, model artifacts, and temp/cache dirs.
- `configs/features.yaml`: feature selection lists, rolling window definitions, filters (e.g., early-down), and defaults.
- `overrides/*.yaml`: manual per-week/game inputs applied during serving before inference (injuries, weather, market edits).

### Modules: responsibilities and I/O

- `src/data/readers.py`
  - Purpose: Thin wrappers around `nflreadpy` with retries, type/schema checks, and optional local caching.
  - Inputs: seasons/weeks list, resource names (schedules, pbp, weekly, rosters, injuries, snaps, depth charts, ids).
  - Outputs: Pandas DataFrames written to Parquet (`data/parquet/<table>/season=<yr>/week=<wk>/*.parquet`).
  - Key contracts: ensure presence of market columns in schedules (`spread_line`, `total_line`, `moneyline`); ensure PBP contains EPA/WP/cp.

- `src/data/transforms.py`
  - Purpose: Cleaning, normalization, ID mapping, and assembling drive-level tables.
  - Inputs: Parquet tables generated by readers; ids from `import_ids`.
  - Outputs: Cleaned tables with normalized team/player IDs; derived drive tables.
  - Key contracts: stable keys for joins (game_id, season, week, team, opponent). No leakage across weeks.

- `src/data/overrides.py`
  - Purpose: Load and apply YAML/JSON overrides to feature rows prior to inference.
  - Inputs: `overrides/*.yaml` files; base feature rows.
  - Outputs: Mutated feature rows with overrides applied (e.g., injury availability, weather, spread edits).
  - Key contracts: deterministic application order; validation of keys (team positions, weather fields, schedule lines).

- `src/features/rolling.py`
  - Purpose: Compute rolling windows (L3/L5/EWMA) for EPA/success/explosive, early-down filters.
  - Inputs: team-game aggregated tables from PBP.
  - Outputs: per team-game rolling columns (e.g., `off_epa_play_l3`, `early_down_pass_epa_l3`).
  - Key contracts: strictly time-ordered windows prior to prediction game.

- `src/features/strategy.py`
  - Purpose: Compute PROE and early-down pass rate.
  - Inputs: PBP with pass probability model features; realized pass rates.
  - Outputs: `proe_*`, `early_down_pass_rate_*` columns at team-game grain.

- `src/features/qb.py`
  - Purpose: QB composites (EPA+CPOE), aDOT; starter mapping via depth charts/snaps.
  - Inputs: PBP with `cp` for CPOE; depth charts; snap counts; injuries.
  - Outputs: `qb_epa_cpoe_l6`, `adot_l5`, and starter-resolved QB features.

- `src/features/trenches.py`
  - Purpose: Pressure proxies; optional ESPN PBWR/PRWR scrape mapping.
  - Inputs: PBP proxies or scraped tables keyed by team/season/week.
  - Outputs: `pressure_allowed_l5`, `pressure_created_l5`, `sack_rate_oe_l5`, optional `pbwr_off`, `prwr_def`.

- `src/features/situational.py`
  - Purpose: Rest/travel/primetime/weather; penalties; OL continuity; turnover luck adjustments.
  - Inputs: schedules, injuries, depth charts, snaps, stadium metadata, weather forecasts (or overrides).
  - Outputs: situational columns listed in the Data Dictionary (e.g., `rest_days`, `wx_*`, `inj_*`, `ol_continuity_index`).

- `src/features/assemble.py`
  - Purpose: Join all engineered features into the modeling table (two rows per game).
  - Inputs: outputs from other feature modules; market priors from schedules.
  - Outputs: modeling Parquet table and/or feature store table; training labels (`label_win`) for historical rows only.
  - Key contracts: no leakage; column order/versioning tracked for modeling.

- `src/models/baseline.py`
  - Purpose: Train baseline logistic model on compact features.
  - Inputs: modeling table; feature list from `configs/features.yaml`.
  - Outputs: serialized scaler/model (`models/baseline.joblib`), feature column order, metrics.

- `src/models/neural_network.py`
  - Purpose: Train multi-branch neural network with 42 features.
  - Inputs: modeling table; rolling features (36), situational features (6).
  - Outputs: serialized model (`models/neural_network.h5`), scaler, feature column order, metrics.
  - Architecture: Rolling branch (64→32→16) + Situational branch (16→8) + Combined (32→16→1).
  - Regularization: BatchNorm, Dropout (0.2-0.3), Early stopping, L2 weight decay.

- `src/models/gbm.py`
  - Purpose: Train GBM/XGBoost/LightGBM ensemble.
  - Inputs: modeling table; extended feature set; CV folds.
  - Outputs: serialized booster, feature importance, CV metrics.

- `src/models/stack.py`
  - Purpose: Blend/stack baseline + neural network + GBM (+ optional Elo-like prior).
  - Inputs: out-of-fold predictions from submodels; meta-features.
  - Outputs: stacked predictions and final model artifact.

- `src/models/calibrate.py`
  - Purpose: Fit isotonic/Platt calibration on validation folds versus moneyline-implied probabilities.
  - Inputs: validation predictions and target probs; mapping utility to/from moneyline odds.
  - Outputs: calibration mapping object; calibration plots/metrics.

- `src/eval/metrics.py`
  - Purpose: Metrics (log-loss, Brier, ECE), reliability curves, slice metrics.
  - Inputs: predictions and labels; slice definitions.
  - Outputs: numerical metrics and optionally plots saved to disk.

- `src/eval/backtest.py`
  - Purpose: Time-series CV harness by week; holdout evaluation.
  - Inputs: modeling table; season/week folds.
  - Outputs: OOF predictions, fold metrics, holdout metrics.

- `src/eval/ablations.py`
  - Purpose: Family-level feature ablations (injury, weather, trenches, market priors).
  - Inputs: feature selection masks; training pipeline hooks.
  - Outputs: delta metrics to guide iteration priorities.

- `src/serve/predict.py`
  - Purpose: CLI for pre-game predictions with manual overrides and explainability.
  - Inputs: teams/date/week; overrides YAML; current feature store snapshot.
  - Outputs: win probabilities for each team, SHAP drivers, and flags (e.g., upset alert).

### Storage layout and formats

- Parquet lake: `data/parquet/<table>/season=<yr>/week=<wk>/*.parquet` (columnar, partitioned).
- Feature store (optional): SQLite/Postgres tables with indices on (`season`, `week`, `team`, `opponent`).
- Models: `models/*.joblib` (or framework-native formats) plus `models/metadata.json` for versions.
- Evaluation artifacts: `data/eval/` for plots and CSV metrics.

### Interfaces and common utilities

- Schema validation helpers: assert presence/type of critical columns (markets, EPA/WP/cp, keys).
- Retry/caching: reader functions should support backoff and local cache keys.
- Time-ordering guard: utilities to slice rows strictly before a given game_id/week for feature windows.
- Odds utilities: convert spread/moneyline to implied probabilities for calibration and benchmarks.

### API layer (FastAPI)

- `src/api/app.py`: FastAPI factory and router registration.
- Routers:
  - `src/api/routers/health.py` → `GET /health/`
  - `src/api/routers/games.py` → `GET /games/?season=&week=`; sources slates from schedules parquet or feature store.
  - `src/api/routers/predict.py` → `POST /predict/`; marshals request overrides, calls `src/serve/predict.py`, returns calibrated probabilities + drivers.
- Contracts: documented in `docs/API.md`. API responses are thin DTOs built from the modeling table outputs and model artifacts.
- Frontend (later): single-page React app consumes these endpoints; no server-side templating.

### Command-line workflows (examples)

- Build data (weekly): readers → transforms → features → assemble modeling table.
- Train baseline: fit logistic, save model + scaler + column order, report metrics.
- Train neural network: fit multi-branch network, save model + scaler + column order, report metrics.
- Train ensemble + calibrate: GBM/XGBoost, stacking, isotonic fit; save artifacts.
- Predict: load latest features, apply overrides, run stack + calibration, output JSON/CSV with SHAP.

### Minimal expected outputs

- Modeling table columns must match the Data Dictionary in `Readme`.
- Saved models must include column order and preprocessing state to ensure stable inference.
- Serving must output: `team_win_prob`, `opp_win_prob`, top SHAP drivers, and any alert flags.


